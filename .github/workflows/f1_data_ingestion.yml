name: F1 Data Ingestion (Manual)

on:
    # Manual trigger only - no schedules
    workflow_dispatch:
        inputs:
            year:
                description: "Year (e.g., 2024, 2023, 2022)"
                required: true
                type: string
                default: "2024"

            event_name:
                description: 'Event name (e.g., ["Italian Grand Prix"], ["Italian Grand Prix", "Monaco Grand Prix"]) - Leave empty for full season'
                required: false
                type: string
                default: "[]"

            session_type:
                description: 'Session type (["R"], ["Q", "S"], ["FP1", "FP2", "FP3"]) - Leave empty for all sessions'
                required: false
                type: string
                default: "[]"

            dry_run:
                description: "Dry run mode (validate only, no actual ingestion)"
                required: true
                type: boolean
                default: false

jobs:
    ingest-f1-data:
        name: Ingest F1 Data
        runs-on: ubuntu-latest
        timeout-minutes: 120 # 2 hours max

        steps:
            - name: 🏁 Job Configuration Summary
              run: |
                  # Determine scope
                  if [ -n "${{ inputs.session_type }}" ]; then
                    SCOPE="Single Session"
                  elif [ -n "${{ inputs.event_name }}" ]; then
                    SCOPE="Full Event (All Sessions)"
                  else
                    SCOPE="Full Season (All Events & Sessions)"
                  fi

                  echo "### 🏎️ F1 Data Ingestion Configuration" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
                  echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
                  echo "| **Year** | ${{ inputs.year }} |" >> $GITHUB_STEP_SUMMARY
                  echo "| **Event** | ${{ inputs.event_name || 'All Events (Full Season)' }} |" >> $GITHUB_STEP_SUMMARY
                  echo "| **Session** | ${{ inputs.session_type || 'All Sessions' }} |" >> $GITHUB_STEP_SUMMARY
                  echo "| **Dry Run** | ${{ inputs.dry_run }} |" >> $GITHUB_STEP_SUMMARY
                  echo "| **Scope** | ${SCOPE} |" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY

                  if [ "${{ inputs.dry_run }}" == "true" ]; then
                    echo "⚠️ **DRY RUN MODE** - No data will be uploaded" >> $GITHUB_STEP_SUMMARY
                  fi

            - name: 📥 Checkout Code
              uses: actions/checkout@v4

            - name: 🐍 Set up Python
              uses: actions/setup-python@v5
              with:
                  python-version: "3.11"
                  cache: "pip"
                  cache-dependency-path: "requirements/dagster.txt"

            - name: 📦 Install Dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements/dagster.txt
                  echo "✅ Dependencies installed"

            - name: 📁 Setup Directories
              run: |
                  mkdir -p /tmp/fastf1_cache
                  mkdir -p /tmp/dagster_home
                  mkdir -p monitoring/logs
                  chmod -R 777 /tmp/fastf1_cache /tmp/dagster_home monitoring/logs
                  echo "✅ Directories created"

            - name: 🔍 Validate Configuration
              env:
                  ENVIRONMENT: production
                  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
                  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                  AWS_REGION: ${{ secrets.AWS_REGION }}
                  POSTGRES_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
                  DAGSTER_POSTGRES_DB_URL: ${{ secrets.DAGSTER_DB_URL }}
              run: |
                  echo "Validating environment configuration..."
                  python -c "
                  import os
                  import sys

                  # Check required secrets
                  required = ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'POSTGRES_DB_URL', 'DAGSTER_POSTGRES_DB_URL']
                  missing = [key for key in required if not os.getenv(key)]

                  if missing:
                      print(f'❌ Missing required secrets: {missing}')
                      sys.exit(1)

                  print('✅ All required secrets present')

                  # Validate inputs
                  year = '${{ inputs.year }}'
                  if not year.isdigit() or len(year) != 4:
                      print(f'❌ Invalid year: {year}')
                      sys.exit(1)

                  print(f'✅ Configuration valid')
                  "
                  echo "✅ Configuration validated"

            - name: 🏎️ Run F1 Data Ingestion
              id: ingestion
              env:
                  # Environment
                  ENVIRONMENT: production

                  # AWS S3
                  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
                  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                  AWS_REGION: ${{ secrets.AWS_REGION }}
                  S3_BUCKET_RAW: pb-f1-raw-data
                  S3_BUCKET_PROCESSED: pb-f1-processed-data

                  # Supabase Database (f1_prod_data schema)
                  POSTGRES_DB_HOST: ${{ secrets.SUPABASE_DB_HOST }}
                  POSTGRES_DB_PORT: 5432
                  POSTGRES_DB_NAME: postgres
                  POSTGRES_DB_USER: ${{ secrets.SUPABASE_DB_USER }}
                  POSTGRES_DB_PASSWORD: ${{ secrets.SUPABASE_DB_PASSWORD }}
                  POSTGRES_DB_URL: ${{ secrets.SUPABASE_DB_URL }}

                  # Dagster metadata (f1_dagster_metadata schema)
                  DAGSTER_POSTGRES_HOST: ${{ secrets.SUPABASE_DB_HOST }}
                  DAGSTER_POSTGRES_PORT: 5432
                  DAGSTER_POSTGRES_DB: postgres
                  DAGSTER_POSTGRES_USER: ${{ secrets.SUPABASE_DB_USER }}
                  DAGSTER_POSTGRES_PASSWORD: ${{ secrets.SUPABASE_DB_PASSWORD }}
                  DAGSTER_POSTGRES_DB_URL: ${{ secrets.DAGSTER_DB_URL }}
                  DAGSTER_HOME: /tmp/dagster_home

                  # FastF1 Configuration
                  FASTF1_CACHE_DIR: /tmp/fastf1_cache
                  FASTF1_CACHE_ENABLED: False
                  FASTF1_LOG_LEVEL: WARNING
                  FASTF1_FORCE_RENEW: False
                  FASTF1_REQUEST_TIMEOUT: 30
                  FASTF1_MAX_RETRIES: 3
                  FASTF1_RETRY_DELAY: 5

                  # Session Types
                  INCLUDE_QUALIFYING: True
                  INCLUDE_SPRINT: True
                  INCLUDE_FREE_PRACTICE: False
                  INCLUDE_TESTING: False

                  # Data Features
                  ENABLE_LAPS: True
                  ENABLE_RACE_CONTROL_MSGS: True
                  ENABLE_WEATHER_DATA: True
                  ENABLE_TELEMETRY: False

                  # Logging
                  LOG_DIR: monitoring/logs
                  LOG_LEVEL: INFO

              run: |
                  echo "Starting F1 data ingestion using run_ingestion.py script..."
                  echo ""

                  # Build command with arguments
                  CMD="python scripts/run_ingestion.py --year ${{ inputs.year }}"

                  # Add optional event name (with quotes if it contains spaces)
                  if [ -n "${{ inputs.event_name }}" ]; then
                    CMD="$CMD --event '${{ inputs.event_name }}'"
                  fi

                  # Add optional session type
                  if [ -n "${{ inputs.session_type }}" ]; then
                    CMD="$CMD --session ${{ inputs.session_type }}"
                  fi

                  # Add dry-run flag if enabled
                  if [ "${{ inputs.dry_run }}" == "true" ]; then
                    CMD="$CMD --dry-run"
                  fi

                  echo "Executing: $CMD"
                  echo ""

                  # Execute the script
                  eval $CMD

            - name: 📊 Upload Logs (on failure)
              if: failure()
              uses: actions/upload-artifact@v4
              with:
                  name: dagster-logs-${{ github.run_id }}
                  path: |
                      monitoring/logs/
                      /tmp/dagster_home/
                  retention-days: 7
                  if-no-files-found: warn

            - name: 📝 Job Summary
              if: always()
              run: |
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "---" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "### 📊 Execution Results" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY

                  if [ "${{ steps.ingestion.outcome }}" == "success" ]; then
                    echo "✅ **Status**: Success" >> $GITHUB_STEP_SUMMARY
                    echo "" >> $GITHUB_STEP_SUMMARY
                    echo "Data has been successfully ingested to:" >> $GITHUB_STEP_SUMMARY
                    echo "- 🪣 **S3 Bucket**: \`pb-f1-raw-data\`" >> $GITHUB_STEP_SUMMARY
                    echo "- 🗄️ **Database**: Supabase (\`f1_prod_data\` schema)" >> $GITHUB_STEP_SUMMARY
                  else
                    echo "❌ **Status**: Failed" >> $GITHUB_STEP_SUMMARY
                    echo "" >> $GITHUB_STEP_SUMMARY
                    echo "Please check the job logs for details." >> $GITHUB_STEP_SUMMARY
                    echo "Logs have been uploaded as artifacts." >> $GITHUB_STEP_SUMMARY
                  fi

                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "**Run ID**: \`${{ github.run_id }}\`" >> $GITHUB_STEP_SUMMARY
                  echo "**Timestamp**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY

    # Retry logic - runs if main job fails
    retry-on-failure:
        name: Retry on Failure (Attempt ${{ matrix.attempt }}/3)
        needs: ingest-f1-data
        if: failure()
        runs-on: ubuntu-latest
        timeout-minutes: 120
        strategy:
            max-parallel: 1
            matrix:
                attempt: [1, 2] # 2 retry attempts after initial failure

        steps:
            - name: ⏳ Wait Before Retry
              run: |
                  WAIT_TIME=$((30 * ${{ matrix.attempt }}))
                  echo "Waiting ${WAIT_TIME} seconds before retry attempt ${{ matrix.attempt }}..."
                  sleep ${WAIT_TIME}

            - name: 📥 Checkout Code
              uses: actions/checkout@v4

            - name: 🐍 Set up Python
              uses: actions/setup-python@v5
              with:
                  python-version: "3.11"
                  cache: "pip"

            - name: 📦 Install Dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements/dagster.txt

            - name: 📁 Setup Directories
              run: |
                  mkdir -p /tmp/fastf1_cache /tmp/dagster_home monitoring/logs
                  chmod -R 777 /tmp/fastf1_cache /tmp/dagster_home monitoring/logs

            - name: 🔄 Retry Ingestion (Attempt ${{ matrix.attempt }})
              env:
                  ENVIRONMENT: production
                  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
                  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                  AWS_REGION: ${{ secrets.AWS_REGION }}
                  S3_BUCKET_RAW: pb-f1-raw-data
                  POSTGRES_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
                  DAGSTER_POSTGRES_DB_URL: ${{ secrets.DAGSTER_DB_URL }}
                  DAGSTER_HOME: /tmp/dagster_home
                  FASTF1_CACHE_DIR: /tmp/fastf1_cache
                  FASTF1_CACHE_ENABLED: False
                  FASTF1_LOG_LEVEL: INFO
                  LOG_DIR: monitoring/logs
                  LOG_LEVEL: INFO
              run: |
                  echo "🔄 Retry attempt ${{ matrix.attempt }} of 3"

                  # Build command
                  CMD="python scripts/run_ingestion.py --year ${{ inputs.year }}"

                  if [ -n "${{ inputs.event_name }}" ]; then
                    CMD="$CMD --event '${{ inputs.event_name }}'"
                  fi

                  if [ -n "${{ inputs.session_type }}" ]; then
                    CMD="$CMD --session ${{ inputs.session_type }}"
                  fi

                  # Execute
                  eval $CMD

            - name: ✅ Retry Successful
              if: success()
              run: |
                  echo "### ✅ Retry Successful (Attempt ${{ matrix.attempt }})" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "Job completed successfully after retry." >> $GITHUB_STEP_SUMMARY

            - name: 📊 Upload Retry Logs (on final failure)
              if: failure() && matrix.attempt == 2
              uses: actions/upload-artifact@v4
              with:
                  name: retry-logs-${{ github.run_id }}-attempt-${{ matrix.attempt }}
                  path: |
                      monitoring/logs/
                      /tmp/dagster_home/
                  retention-days: 7
                  if-no-files-found: warn

    # Notification job - runs after all retries
    notify-final-status:
        name: Send Notification
        needs: [ingest-f1-data, retry-on-failure]
        if: always()
        runs-on: ubuntu-latest

        steps:
            - name: 📧 Create Notification Summary
              run: |
                  echo "### 📬 Final Status Notification" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY

                  if [ "${{ needs.ingest-f1-data.result }}" == "success" ]; then
                    echo "✅ Job succeeded on first attempt" >> $GITHUB_STEP_SUMMARY
                  elif [ "${{ needs.retry-on-failure.result }}" == "success" ]; then
                    echo "✅ Job succeeded after retry" >> $GITHUB_STEP_SUMMARY
                  else
                    echo "❌ Job failed after all retry attempts" >> $GITHUB_STEP_SUMMARY
                    echo "" >> $GITHUB_STEP_SUMMARY
                    echo "**Action Required**: Please check the logs and investigate the failure." >> $GITHUB_STEP_SUMMARY
                  fi

                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "**Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
                  echo "**Run**: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
