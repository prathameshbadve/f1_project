name: F1 Data Ingestion (Manual)

on:
  # Manual trigger only - no schedules
  workflow_dispatch:
    inputs:
      year:
        description: "Year (e.g., 2024, 2023, 2022)"
        required: true
        type: string
        default: "2024"

      event_name:
        description: 'Event name(s) - space-separated for multiple. Example: "Italian Grand Prix" or "Italian Grand Prix" "Monaco Grand Prix". Leave empty for all events.'
        required: false
        type: string
        default: ""

      session_type:
        description: 'Session type(s) - space-separated for multiple. Example: R or R Q S or "R" "Q" "S". Leave empty for all sessions.'
        required: false
        type: string
        default: ""

      dry_run:
        description: "Dry run mode (validate only, no actual ingestion)"
        required: true
        type: boolean
        default: false

jobs:
  ingest-f1-data:
    name: Ingest F1 Data
    runs-on: ubuntu-latest
    timeout-minutes: 120 # 2 hours max

    steps:
      - name: ðŸ Job Configuration Summary
        run: |
          # Store inputs in variables with proper quoting
          EVENT_INPUT='${{ inputs.event_name }}'
          SESSION_INPUT='${{ inputs.session_type }}'

          # Determine scope
          if [ -n "$SESSION_INPUT" ] && [ -n "$EVENT_INPUT" ]; then
            # Count number of items
            EVENT_COUNT=$(echo "$EVENT_INPUT" | tr ',' '\n' | wc -l)
            SESSION_COUNT=$(echo "$SESSION_INPUT" | tr ',' '\n' | wc -l)
            
            if [ "$EVENT_COUNT" -eq 1 ] && [ "$SESSION_COUNT" -eq 1 ]; then
              SCOPE="Mode 1: Single Session"
            elif [ "$EVENT_COUNT" -eq 1 ] && [ "$SESSION_COUNT" -gt 1 ]; then
              SCOPE="Mode 2: Multiple Sessions from One Event"
            elif [ "$EVENT_COUNT" -gt 1 ] && [ "$SESSION_COUNT" -eq 1 ]; then
              SCOPE="Mode 3: One Session Type from Multiple Events"
            else
              SCOPE="Mode 4: Multiple Session Types from Multiple Events"
            fi
          elif [ -n "$EVENT_INPUT" ] && [ -z "$SESSION_INPUT" ]; then
            EVENT_COUNT=$(echo "$EVENT_INPUT" | tr ',' '\n' | wc -l)
            if [ "$EVENT_COUNT" -eq 1 ]; then
              SCOPE="Mode 5: All Sessions from One Event"
            else
              SCOPE="Mode 6: All Sessions from Multiple Events"
            fi
          elif [ -z "$EVENT_INPUT" ] && [ -n "$SESSION_INPUT" ]; then
            SESSION_COUNT=$(echo "$SESSION_INPUT" | tr ',' '\n' | wc -l)
            if [ "$SESSION_COUNT" -eq 1 ]; then
              SCOPE="Mode 7: One Session Type from All Events"
            else
              SCOPE="Mode 8: Multiple Session Types from All Events"
            fi
          else
            SCOPE="Mode 9: Whole Season (All Events & Sessions)"
          fi

          echo "### ðŸŽï¸ F1 Data Ingestion Configuration" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| **Year** | ${{ inputs.year }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Event(s)** | ${EVENT_INPUT:-All Events (Full Season)} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Session(s)** | ${SESSION_INPUT:-All Sessions} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Dry Run** | ${{ inputs.dry_run }} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Scope** | ${SCOPE} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ inputs.dry_run }}" == "true" ]; then
            echo "âš ï¸ **DRY RUN MODE** - No data will be uploaded" >> $GITHUB_STEP_SUMMARY
          fi

      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: "requirements/dagster.txt"

      - name: ðŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements/dagster.txt
          echo "âœ… Dependencies installed"

      - name: ðŸ“ Setup Directories
        run: |
          mkdir -p /tmp/fastf1_cache
          mkdir -p /tmp/dagster_home
          mkdir -p monitoring/logs
          chmod -R 777 /tmp/fastf1_cache /tmp/dagster_home monitoring/logs
          echo "âœ… Directories created"

      - name: ðŸ” Validate Configuration
        env:
          ENVIRONMENT: production
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          POSTGRES_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
          DAGSTER_POSTGRES_DB_URL: ${{ secrets.DAGSTER_DB_URL }}
        run: |
          echo "ðŸ” Validating environment configuration..."

          # Check AWS credentials
          if [ -z "$AWS_ACCESS_KEY_ID" ] || [ -z "$AWS_SECRET_ACCESS_KEY" ]; then
            echo "âŒ AWS credentials not configured"
            exit 1
          fi

          # Check database URL
          if [ -z "$POSTGRES_DB_URL" ]; then
            echo "âŒ Database URL not configured"
            exit 1
          fi

          echo "âœ… Configuration validated"

      - name: ðŸŽï¸ Run F1 Data Ingestion
        id: ingestion
        env:
          # Python path - CRITICAL FIX
          PYTHONPATH: ${{ github.workspace }}

          # Environment
          ENVIRONMENT: production

          # AWS Configuration
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET_RAW: pb-f1-raw-data

          # Database Configuration
          POSTGRES_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
          DAGSTER_POSTGRES_DB_URL: ${{ secrets.DAGSTER_DB_URL }}
          DAGSTER_HOME: /tmp/dagster_home

          # FastF1 Configuration
          FASTF1_CACHE_DIR: /tmp/fastf1_cache
          FASTF1_CACHE_ENABLED: False
          FASTF1_LOG_LEVEL: INFO

          # Logging
          LOG_DIR: monitoring/logs
          LOG_LEVEL: INFO
        run: |
          echo "ðŸš€ Starting F1 data ingestion using run_ingestion.py script..."
          echo ""

          # Verify PYTHONPATH
          echo "PYTHONPATH: $PYTHONPATH"
          echo "Current directory: $(pwd)"
          echo ""

          # Store inputs in variables
          EVENT_INPUT='${{ inputs.event_name }}'
          SESSION_INPUT='${{ inputs.session_type }}'

          # Build base command
          CMD="python scripts/data_ingestion/run_ingestion.py --year ${{ inputs.year }}"

          # Add event names if provided
          if [ -n "$EVENT_INPUT" ]; then
            CMD="$CMD --event $EVENT_INPUT"
          fi

          # Add session types if provided
          if [ -n "$SESSION_INPUT" ]; then
            CMD="$CMD --session $SESSION_INPUT"
          fi

          # Add dry-run flag if enabled
          if [ "${{ inputs.dry_run }}" == "true" ]; then
            CMD="$CMD --dry-run"
          fi

          echo "Executing: $CMD"
          echo ""

          # Execute the script
          eval $CMD

      - name: ðŸ“Š Upload Logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: dagster-logs-${{ github.run_id }}
          path: |
            monitoring/logs/
            /tmp/dagster_home/
          retention-days: 7
          if-no-files-found: ignore

      - name: ðŸ“ Job Summary
        if: always()
        run: |
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“Š Execution Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.ingestion.outcome }}" == "success" ]; then
            echo "âœ… **Status**: Success" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Data has been successfully ingested to:" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸª£ **S3 Bucket**: \`pb-f1-raw-data\`" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ—„ï¸ **Database**: Supabase (\`f1_prod_data\` schema)" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Status**: Failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Please check the job logs for details." >> $GITHUB_STEP_SUMMARY
            if [ -d "monitoring/logs" ] && [ "$(ls -A monitoring/logs 2>/dev/null)" ]; then
              echo "Logs have been uploaded as artifacts." >> $GITHUB_STEP_SUMMARY
            fi
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID**: \`${{ github.run_id }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY

  # Retry logic - runs if main job fails
  retry-on-failure:
    name: Retry on Failure (Attempt ${{ matrix.attempt }}/3)
    needs: ingest-f1-data
    if: failure()
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      max-parallel: 1
      matrix:
        attempt: [1, 2] # 2 retry attempts after initial failure

    steps:
      - name: â³ Wait Before Retry
        run: |
          WAIT_TIME=$((30 * ${{ matrix.attempt }}))
          echo "Waiting ${WAIT_TIME} seconds before retry attempt ${{ matrix.attempt }}..."
          sleep ${WAIT_TIME}

      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: ðŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements/dagster.txt

      - name: ðŸ“ Setup Directories
        run: |
          mkdir -p /tmp/fastf1_cache /tmp/dagster_home monitoring/logs
          chmod -R 777 /tmp/fastf1_cache /tmp/dagster_home monitoring/logs

      - name: ðŸ”„ Retry Ingestion (Attempt ${{ matrix.attempt }})
        env:
          # Python path - CRITICAL FIX
          PYTHONPATH: ${{ github.workspace }}

          ENVIRONMENT: production
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET_RAW: pb-f1-raw-data
          POSTGRES_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
          DAGSTER_POSTGRES_DB_URL: ${{ secrets.DAGSTER_DB_URL }}
          DAGSTER_HOME: /tmp/dagster_home
          FASTF1_CACHE_DIR: /tmp/fastf1_cache
          FASTF1_CACHE_ENABLED: False
          FASTF1_LOG_LEVEL: INFO
          LOG_DIR: monitoring/logs
          LOG_LEVEL: INFO
        run: |
          echo "ðŸ”„ Retry attempt ${{ matrix.attempt }} of 3"

          # Store inputs in variables
          EVENT_INPUT='${{ inputs.event_name }}'
          SESSION_INPUT='${{ inputs.session_type }}'

          # Build command
          CMD="python scripts/data_ingestion/run_ingestion.py --year ${{ inputs.year }}"

          # Add event names if provided
          if [ -n "$EVENT_INPUT" ]; then
            CMD="$CMD --event $EVENT_INPUT"
          fi

          # Add session types if provided
          if [ -n "$SESSION_INPUT" ]; then
            CMD="$CMD --session $SESSION_INPUT"
          fi

          # Execute
          eval $CMD

      - name: âœ… Retry Successful
        if: success()
        run: |
          echo "### âœ… Retry Successful (Attempt ${{ matrix.attempt }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Job completed successfully after retry." >> $GITHUB_STEP_SUMMARY

      - name: ðŸ“Š Upload Retry Logs (on final failure)
        if: failure() && matrix.attempt == 2
        uses: actions/upload-artifact@v4
        with:
          name: retry-logs-${{ github.run_id }}-attempt-${{ matrix.attempt }}
          path: |
            monitoring/logs/
            /tmp/dagster_home/
          retention-days: 7
          if-no-files-found: ignore

  # Notification job - runs after all retries
  notify-final-status:
    name: Send Notification
    needs: [ingest-f1-data, retry-on-failure]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: ðŸ“§ Create Notification Summary
        run: |
          echo "### ðŸ“¬ Final Status Notification" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.ingest-f1-data.result }}" == "success" ]; then
            echo "âœ… Job succeeded on first attempt" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.retry-on-failure.result }}" == "success" ]; then
            echo "âœ… Job succeeded after retry" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Job failed after all retry attempts" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Action Required**: Please check the logs and investigate the failure." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "**Run**: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
