name: F1 Data Ingestion (Manual)

on:
    # Manual trigger only - no schedules
    workflow_dispatch:
        inputs:
            year:
                description: "Year (e.g., 2024, 2023, 2022)"
                required: true
                type: string
                default: "2024"

            event_name:
                description: 'Event name (e.g., "Italian Grand Prix", "Monaco Grand Prix") - Leave empty for full season'
                required: false
                type: string
                default: ""

            session_type:
                description: "Session type (R, Q, S, SS, SQ, FP1, FP2, FP3) - Leave empty for all sessions"
                required: false
                type: string
                default: ""

            dry_run:
                description: "Dry run mode (validate only, no actual ingestion)"
                required: true
                type: boolean
                default: false

jobs:
    ingest-f1-data:
        name: Ingest F1 Data
        runs-on: ubuntu-latest
        timeout-minutes: 120 # 2 hours max

        steps:
            - name: 🏁 Job Configuration Summary
              run: |
                  # Determine scope

                  if [ -n "${{ inputs.session_type }}" ]; then
                    SCOPE="Single Session"
                  elif [ -n "${{ inputs.event_name }}" ]; then
                    SCOPE="Full Event (All Sessions)"
                  else
                    SCOPE="Full Season (All Events & Sessions)"
                  fi

                  echo "### 🏎️ F1 Data Ingestion Configuration" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
                  echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
                  echo "| **Year** | ${{ inputs.year }} |" >> $GITHUB_STEP_SUMMARY
                  echo "| **Event** | ${{ inputs.event_name || 'All Events (Full Season)' }} |" >> $GITHUB_STEP_SUMMARY
                  echo "| **Session** | ${{ inputs.session_type || 'All Sessions' }} |" >> $GITHUB_STEP_SUMMARY
                  echo "| **Dry Run** | ${{ inputs.dry_run }} |" >> $GITHUB_STEP_SUMMARY
                  echo "| **Scope** | ${SCOPE} |" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY

                  if [ "${{ inputs.dry_run }}" == "true" ]; then
                    echo "⚠️ **DRY RUN MODE** - No data will be uploaded" >> $GITHUB_STEP_SUMMARY
                  fi

            - name: 📥 Checkout Code
              uses: actions/checkout@v4

            - name: 🐍 Set up Python
              uses: actions/setup-python@v5
              with:
                  python-version: "3.11"
                  cache: "pip"
                  cache-dependency-path: "requirements/dagster.txt"

            - name: 📦 Install Dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements/dagster.txt
                  echo "✅ Dependencies installed"

            - name: 📁 Setup Directories
              run: |
                  mkdir -p /tmp/fastf1_cache
                  mkdir -p /tmp/dagster_home
                  mkdir -p /tmp/dagster/logs
                  chmod -R 777 /tmp/fastf1_cache /tmp/dagster_home /tmp/dagster/logs
                  echo "✅ Directories created"

            - name: 🔍 Validate Configuration
              env:
                  ENVIRONMENT: production
                  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
                  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                  AWS_REGION: ${{ secrets.AWS_REGION }}
                  POSTGRES_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
                  DAGSTER_POSTGRES_DB_URL: ${{ secrets.DAGSTER_DB_URL }}
              run: |
                  echo "Validating environment configuration..."
                  python -c "
                  import os
                  import sys

                  # Check required secrets
                  required = ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'POSTGRES_DB_URL', 'DAGSTER_POSTGRES_DB_URL']
                  missing = [key for key in required if not os.getenv(key)]

                  if missing:
                      print(f'❌ Missing required secrets: {missing}')
                      sys.exit(1)

                  print('✅ All required secrets present')

                  # Validate inputs
                  year = '${{ inputs.year }}'
                  if not year.isdigit() or len(year) != 4:
                      print(f'❌ Invalid year: {year}')
                      sys.exit(1)

                  print(f'✅ Configuration valid')
                  "
                  echo "✅ Configuration validated"

            - name: 🏎️ Run F1 Data Ingestion
              id: ingestion
              env:
                  # Environment
                  ENVIRONMENT: production

                  # AWS S3
                  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
                  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                  AWS_REGION: ${{ secrets.AWS_REGION }}
                  S3_BUCKET_RAW: pb-f1-raw-data
                  S3_BUCKET_PROCESSED: pb-f1-processed-data

                  # Supabase Database (f1_prod_data schema)
                  POSTGRES_DB_HOST: ${{ secrets.SUPABASE_DB_HOST }}
                  POSTGRES_DB_PORT: 5432
                  POSTGRES_DB_NAME: postgres
                  POSTGRES_DB_USER: ${{ secrets.SUPABASE_DB_USER }}
                  POSTGRES_DB_PASSWORD: ${{ secrets.SUPABASE_DB_PASSWORD }}
                  POSTGRES_DB_URL: ${{ secrets.SUPABASE_DB_URL }}

                  # Dagster metadata (f1_dagster_metadata schema)
                  DAGSTER_POSTGRES_HOST: ${{ secrets.SUPABASE_DB_HOST }}
                  DAGSTER_POSTGRES_PORT: 5432
                  DAGSTER_POSTGRES_DB: postgres
                  DAGSTER_POSTGRES_USER: ${{ secrets.SUPABASE_DB_USER }}
                  DAGSTER_POSTGRES_PASSWORD: ${{ secrets.SUPABASE_DB_PASSWORD }}
                  DAGSTER_POSTGRES_DB_URL: ${{ secrets.DAGSTER_DB_URL }}
                  DAGSTER_HOME: /tmp/dagster_home

                  # FastF1 Configuration
                  FASTF1_CACHE_DIR: /tmp/fastf1_cache
                  FASTF1_CACHE_ENABLED: False
                  FASTF1_LOG_LEVEL: WARNING
                  FASTF1_FORCE_RENEW: False
                  FASTF1_REQUEST_TIMEOUT: 30
                  FASTF1_MAX_RETRIES: 3
                  FASTF1_RETRY_DELAY: 5

                  # Session Types
                  INCLUDE_QUALIFYING: True
                  INCLUDE_SPRINT: True
                  INCLUDE_FREE_PRACTICE: False
                  INCLUDE_TESTING: False

                  # Data Features
                  ENABLE_LAPS: True
                  ENABLE_RACE_CONTROL_MSGS: True
                  ENABLE_WEATHER_DATA: True
                  ENABLE_TELEMETRY: False

                  # Logging
                  LOG_DIR: /tmp/monitoring/logs
                  LOG_LEVEL: INFO

              run: |
                  echo "Starting F1 data ingestion..."

                  # Build run config based on inputs
                  YEAR="${{ inputs.year }}"
                  EVENT_NAME="${{ inputs.event_name }}"
                  SESSION_TYPE="${{ inputs.session_type }}"
                  DRY_RUN="${{ inputs.dry_run }}"

                  # Create Python script to run the job
                  cat > run_ingestion.py << 'EOFPYTHON'
                  import os
                  import sys
                  from dagster import DagsterInstance, execute_job
                  from dagster_project import defs

                  # Get inputs
                  year = int("${{ inputs.year }}")
                  event_name = "${{ inputs.event_name }}" or None
                  session_type = "${{ inputs.session_type }}" or None
                  dry_run = "${{ inputs.dry_run }}" == "true"

                  print(f"\n{'='*60}")
                  print(f"F1 Data Ingestion Job")
                  print(f"{'='*60}")
                  print(f"Year: {year}")
                  print(f"Event: {event_name or 'All Events'}")
                  print(f"Session: {session_type or 'All Sessions'}")
                  print(f"Dry Run: {dry_run}")
                  print(f"{'='*60}\n")

                  if dry_run:
                      print("🔍 DRY RUN MODE - Validation Only")
                      print("\nThis would ingest:")
                      if session_type:
                          print(f"  ✓ Single session: {year} {event_name} {session_type}")
                      elif event_name:
                          print(f"  ✓ All sessions from: {year} {event_name}")
                      else:
                          print(f"  ✓ Full season: All events from {year}")
                      print("\n✅ Dry run validation complete - no data ingested")
                      sys.exit(0)

                  # Get the configurable job
                  job = defs.get_job_def('f1_configurable_session_job')

                  # Build config
                  config = {
                      'ops': {
                          'f1_session_configurable': {
                              'config': {
                                  'year': year,
                              }
                          }
                      }
                  }

                  # Add optional parameters
                  if event_name:
                      config['ops']['f1_session_configurable']['config']['event_name'] = event_name
                  if session_type:
                      config['ops']['f1_session_configurable']['config']['session_type'] = session_type

                  print(f"Executing job with config: {config}\n")

                  # Execute the job
                  try:
                      result = execute_job(
                          job,
                          instance=DagsterInstance.get(),
                          run_config=config
                      )
                      
                      if result.success:
                          print("\n✅ Job completed successfully!")
                          sys.exit(0)
                      else:
                          print("\n❌ Job failed!")
                          sys.exit(1)
                          
                  except Exception as e:
                      print(f"\n❌ Error executing job: {e}")
                      import traceback
                      traceback.print_exc()
                      sys.exit(1)
                  EOFPYTHON

                  # Run the ingestion
                  python run_ingestion.py

            - name: 📊 Upload Logs (on failure)
              if: failure()
              uses: actions/upload-artifact@v4
              with:
                  name: dagster-logs-${{ github.run_id }}
                  path: |
                      monitoring/logs/
                      /tmp/dagster_home/
                  retention-days: 7
                  if-no-files-found: warn

            - name: 📝 Job Summary
              if: always()
              run: |
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "---" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "### 📊 Execution Results" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY

                  if not [ "${{ inputs.dry_run }}" ]; then
                    if [ "${{ steps.ingestion.outcome }}" == "success" ]; then
                        echo "✅ **Status**: Success" >> $GITHUB_STEP_SUMMARY
                        echo "" >> $GITHUB_STEP_SUMMARY
                        echo "Data has been successfully ingested to:" >> $GITHUB_STEP_SUMMARY
                        echo "- 🪣 **S3 Bucket**: \`pb-f1-raw-data\`" >> $GITHUB_STEP_SUMMARY
                        echo "- 🗄️ **Database**: Supabase (\`f1_prod_data\` schema)" >> $GITHUB_STEP_SUMMARY
                    else
                        echo "❌ **Status**: Failed" >> $GITHUB_STEP_SUMMARY
                        echo "" >> $GITHUB_STEP_SUMMARY
                        echo "Please check the job logs for details." >> $GITHUB_STEP_SUMMARY
                        echo "Logs have been uploaded as artifacts." >> $GITHUB_STEP_SUMMARY
                    fi
                  else
                    echo "✅ Dry Run Completed" >> $GITHUB_STEP_SUMMARY
                    echo "" >> $GITHUB_STEP_SUMMARY

                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "**Run ID**: \`${{ github.run_id }}\`" >> $GITHUB_STEP_SUMMARY
                  echo "**Timestamp**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY

    # Retry logic - runs if main job fails
    retry-on-failure:
        name: Retry on Failure (Attempt ${{ matrix.attempt }}/3)
        needs: ingest-f1-data
        if: failure()
        runs-on: ubuntu-latest
        timeout-minutes: 120
        strategy:
            max-parallel: 1
            matrix:
                attempt: [1, 2] # 2 retry attempts after initial failure

        steps:
            - name: ⏳ Wait Before Retry
              run: |
                  WAIT_TIME=$((30 * ${{ matrix.attempt }}))
                  echo "Waiting ${WAIT_TIME} seconds before retry attempt ${{ matrix.attempt }}..."
                  sleep ${WAIT_TIME}

            - name: 📥 Checkout Code
              uses: actions/checkout@v4

            - name: 🐍 Set up Python
              uses: actions/setup-python@v5
              with:
                  python-version: "3.11"
                  cache: "pip"

            - name: 📦 Install Dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -r requirements/dagster.txt

            - name: 📁 Setup Directories
              run: |
                  mkdir -p /tmp/fastf1_cache /tmp/dagster_home /tmp/dagster/logs
                  chmod -R 777 /tmp/fastf1_cache /tmp/dagster_home /tmp/dagster/logs

            - name: 🔄 Retry Ingestion (Attempt ${{ matrix.attempt }})
              env:
                  ENVIRONMENT: production
                  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
                  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                  AWS_REGION: ${{ secrets.AWS_REGION }}
                  S3_BUCKET_RAW: pb-f1-raw-data
                  POSTGRES_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
                  DAGSTER_POSTGRES_DB_URL: ${{ secrets.DAGSTER_DB_URL }}
                  DAGSTER_HOME: /tmp/dagster_home
                  FASTF1_CACHE_DIR: /tmp/fastf1_cache
                  FASTF1_CACHE_ENABLED: False
                  FASTF1_LOG_LEVEL: INFO
                  LOG_DIR: /tmp/dagster/logs
                  LOG_LEVEL: INFO
              run: |
                  echo "🔄 Retry attempt (${{ matrix.attempt }}) of 3"

                  # Same ingestion logic as main job
                  cat > run_ingestion.py << 'EOFPYTHON'
                  import sys
                  from dagster import DagsterInstance, execute_job
                  from dagster_project import defs

                  year = int("${{ inputs.year }}")
                  event_name = "${{ inputs.event_name }}" or None
                  session_type = "${{ inputs.session_type }}" or None

                  job = defs.get_job_def('f1_configurable_session_job')

                  config = {'ops': {'f1_session_configurable': {'config': {'year': year}}}}
                  if event_name:
                      config['ops']['f1_session_configurable']['config']['event_name'] = event_name
                  if session_type:
                      config['ops']['f1_session_configurable']['config']['session_type'] = session_type

                  result = execute_job(job, instance=DagsterInstance.get(), run_config=config)
                  sys.exit(0 if result.success else 1)
                  EOFPYTHON

                  python run_ingestion.py

            - name: ✅ Retry Successful
              if: success()
              run: |
                  echo "### ✅ Retry Successful (Attempt ${{ matrix.attempt }})" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "Job completed successfully after retry." >> $GITHUB_STEP_SUMMARY

            - name: 📊 Upload Retry Logs (on final failure)
              if: failure() && matrix.attempt == 2
              uses: actions/upload-artifact@v4
              with:
                  name: retry-logs-${{ github.run_id }}-attempt-${{ matrix.attempt }}
                  path: |
                      monitoring/logs/
                      /tmp/dagster_home/
                  retention-days: 7
                  if-no-files-found: warn

    # Notification job - runs after all retries
    notify-final-status:
        name: Send Notification
        needs: [ingest-f1-data, retry-on-failure]
        if: always()
        runs-on: ubuntu-latest

        steps:
            - name: 📧 Create Notification Summary
              run: |
                  echo "### 📬 Final Status Notification" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY

                  if [ "${{ needs.ingest-f1-data.result }}" == "success" ]; then
                    echo "✅ Job succeeded on first attempt" >> $GITHUB_STEP_SUMMARY
                  elif [ "${{ needs.retry-on-failure.result }}" == "success" ]; then
                    echo "✅ Job succeeded after retry" >> $GITHUB_STEP_SUMMARY
                  else
                    echo "❌ Job failed after all retry attempts" >> $GITHUB_STEP_SUMMARY
                    echo "" >> $GITHUB_STEP_SUMMARY
                    echo "**Action Required**: Please check the logs and investigate the failure." >> $GITHUB_STEP_SUMMARY
                  fi

                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "**Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
                  echo "**Run**: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
